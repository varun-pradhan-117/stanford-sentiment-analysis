{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import BertTokenizer, cached_path\n",
    "from utils.transformers import TransformerWithClfHeadAndAdapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"transformer_results\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = torch.load(cached_path(os.path.join(model_path, \"model_training_args.bin\")))\n",
    "model = TransformerWithClfHeadAndAdapters(config[\"config\"],\n",
    "                                          config[\"config_ft\"]).to(device)\n",
    "state_dict = torch.load(cached_path(os.path.join(model_path, \"model_weights.pth\")),\n",
    "                        map_location=device)\n",
    "\n",
    "model.load_state_dict(state_dict)   # Load model state dict\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)  # Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_token = tokenizer.vocab['[CLS]']  # classifier token\n",
    "pad_token = tokenizer.vocab['[PAD]']  # pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(inputs):\n",
    "    # Encode text as IDs using the BertTokenizer\n",
    "    return list(tokenizer.convert_tokens_to_ids(o) for o in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = config['config'].num_max_positions  # Max length from trained model\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie showcased the true ability of The Rock as a phenomenal actor.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This movie showcased the true ability of The Rock as a phenomenal actor.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'movie', 'showcased', 'the', 'true', 'ability', 'of', 'The', 'Rock', 'as', 'a', 'phenomena', '##l', 'actor', '.']\n",
      "[1188, 2523, 24980, 1103, 2276, 2912, 1104, 1109, 2977, 1112, 170, 14343, 1233, 2811, 119, 101]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.tokenize(text)\n",
    "if len(inputs) >= max_length:\n",
    "    inputs = inputs[:max_length - 1]\n",
    "ids = encode(inputs) + [clf_token]\n",
    "print(inputs)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():   # Disable backprop\n",
    "    tensor = torch.tensor(ids, dtype=torch.long).to(device)\n",
    "    tensor_reshaped = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor_reshaped.transpose(0, 1).contiguous()  # to shape [seq length, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask=(tensor_in == clf_token),\n",
    "                   padding_mask=(tensor_reshaped == pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, _ = torch.max(logits, 0)\n",
    "val = F.softmax(val, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities:  [6.9524435e-04 5.4976675e-03 2.4256413e-01 7.5099933e-01 2.4359378e-04] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Class probabilities: \", val, type(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prediction for text example:  4\n"
     ]
    }
   ],
   "source": [
    "pred = int(val.argmax()) + 1\n",
    "print(\"Class prediction for text example: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerWithClfHeadAndAdapters(\n",
      "  (transformer): Transformer(\n",
      "    (tokens_embeddings): Embedding(28996, 410)\n",
      "    (position_embeddings): Embedding(256, 410)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (attentions): ModuleList(\n",
      "      (0): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (1): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (2): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (3): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (4): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (5): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (6): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (7): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (8): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (9): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (10): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (11): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (12): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (13): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (14): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "      (15): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=410, out_features=410, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (feed_forwards): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (8): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (9): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (10): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (11): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (12): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (13): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (14): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "      (15): Sequential(\n",
      "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2100, out_features=410, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norms_1): ModuleList(\n",
      "      (0): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (1): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (2): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (3): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (4): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (5): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (6): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (7): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (8): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (9): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (10): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (11): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (12): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (13): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (14): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (15): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_norms_2): ModuleList(\n",
      "      (0): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (1): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (2): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (3): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (4): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (5): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (6): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (7): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (8): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (9): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (10): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (11): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (12): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (13): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (14): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "      (15): LayerNorm((410,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): Linear(in_features=410, out_features=5, bias=True)\n",
      ") torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "print(model,tensor_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6394067025e7b58be91336a97fabd7e0edf63c22e0826aa98dafbee925a98393"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('pytorch-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
